---
title: "Oligo analysis script v2 (adapted for umierrorcorrect)"
author: "Stefan Filges"
date: '`r format(Sys.Date())`'
output: 
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: yes
    theme: sandstone
    highlight: tango
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup

### User-defined variables

Set the working directory "main" and import plotting functions from the helper functions
file. 

**NOTE!** For this to work you need to specify the location of the 'helper_functions_oligo.R'
scripts. It contains a number of functions that will be used throughout this
analysis.

```{r}
# Should individual files with VAF be printed?
print.individual.files = FALSE

# Select consensus threshold to use for analysis
consensus_cutoff <- 10

# Create table of sample directory paths
# First define the directory containing UMIErrrorCorrect output files
sample_dir = '/Users/stefan/Documents/GitHub/OligoAnalysis/data/umierrorcorrect'

# Load helper functions, change location if needed
source('/Users/stefan/Documents/GitHub/OligoAnalysis/src/helper_functions_oligo.R')
```

### Loading packages

We begin by importing the required packages, and if necessary installing them.

```{r, message=FALSE, warning=FALSE}
# CRAN packages to be installed with checkPackages
packages <- c('tidyverse', 'gridExtra', 'RColorBrewer', 'stringr', 'tis')

# Check and if needed install CRAN packages
checkPackages(requiredPackages = packages)

# If needed, install BiocManager for downloading packages from Bioconductor
if (!require("BiocManager", quietly = TRUE)){
  install.packages("BiocManager")
}

# If needed, install Rsamtools from Bioconductor
if (!require("Rsamtools", quietly = TRUE)){
  BiocManager::install("Rsamtools")
}

# Load packages
library(tidyverse)
library(Rsamtools)
library(EnvStats)
library(Hmisc)
library(ggpmisc)
```

### Importing data

#### Setting up the file structure

We then load the paths to the UMIErrorCorrect output folders into the samplePaths 
variable and create a data frame "sampleNames", that contains the sample metadata based on 
the folder name.

**NOTE!**
```
Sample folders generated with UMIErrorCorrect need to have a specific format or else
the code below needs to be changed for the script to run. Each folder needs to contain
the following information the the same order, separated by underscores:

Manufacturer_InsertType_Purification_Batch_Replicate
```
One relatively easy  way to get this format is to use it on the sample sheet used for 
demultiplexing the sequencing run. Demultiplexed fastq files will append additional
information such as Lane and Read such that the final fastq would look somewhat like

'Manufacturer_InsertType_Purification_Batch_Replicate_L001_R1_001.fastq.gz'

Use a shell script or other to batch process multiple fastqs at once using 
UMIErrorCorrect and use that same script to trim '_L001_R1_001.fastq.gz' from
the output folder names, thus resulting in intended format.

At this stage, also define the "consensus_cutoff" variable which sets the minimum
UMI family size to keep. Somewhere between 3 and 10 will usually be appropriate.

Lastly, a new tibble is initialized into which all consensus table will be loaded 
in the next step.

```{r, warning=FALSE}
# Get full paths for each sample folder
samplePaths <- list.dirs(
  path = sample_dir,
  recursive = FALSE
)

# Create data frame with sample metadata by listing all samples in the previously
# defined samplePaths
sampleNames <- list.dirs(
  path = sample_dir,
  recursive = FALSE,
  full.names = FALSE
)

# Split the sample names into columns by the delimiter '_'
sampleNames <- as.data.frame(do.call(rbind, strsplit(sampleNames, '_')))

# set column names of meta data
colnames(sampleNames) <- c('Manufacturer', 'InsertType', 'Purification', 'Batch', 'Replicate')

# add sample paths to the data frame
sampleNames$File <- samplePaths

# print first few rows and verify that information is correct
print(head(sampleNames))
```

#### Import files

For each sample the cons files are loaded and metadata is added to the 
consensus table for filtering later on. The metadata is derived from the 
folder name and used for filtering of samples downstream.

```{r, warning=FALSE, echo=FALSE, message=FALSE}
# Initialize consensus table object
cons.table = tibble()

# Get cons file for each UMIErrorCorrect output folder 'i'
for(i in 1:nrow(sampleNames)){

  # Get absolute path to sample 'i'
  samplePath <- sampleNames$File[i]

  # List all files in the UMIErrorCorrect data folder ending with '_cons.tsv'
  consFile <- list.files(
    path = samplePath, 
    pattern = '_cons.tsv$',
    full.names = TRUE
  )
  
  # Read sample data
  cons.table.temp <- readr::read_delim(
    file = consFile,
    delim = '\t'
  )
  
  # Add sample meta data, taken from the folder name, to the consensus table
  cons.table.temp$Manufacturer <- sampleNames$Manufacturer[i]
  cons.table.temp$InsertType <- sampleNames$InsertType[i]
  cons.table.temp$Purification <- sampleNames$Purification[i]
  cons.table.temp$Batch <- sampleNames$Batch[i]
  cons.table.temp$Replicate <- sampleNames$Replicate[i]

  cons.table <- cons.table %>% 
    dplyr::bind_rows(cons.table.temp)
}

# Get cons0 and consX, where X is chosen by the user in the consensus_cutoff variable
# Discard all other consensus levels and all rows without an assay name.
cons.table <- cons.table %>% dplyr::filter(
  cons.table$`Consensus group size` %in% c(0, consensus_cutoff),
  !is.na(cons.table$Name)
)
```

If necessary, re-code factor variables for clarity or better plotting, e.g.
shortening names. This part may *not* always be necessary if files are named
consistently from the start. If no factors need to be re-coded, either delete this 
code block or set eval=FALSE.

```{r, warning=FALSE, echo=FALSE, message=FALSE, eval=TRUE}
# TODO change or remove this part as necessary. 
# Change names of factor variables. 
# Recode manufacturers names from file anmes to short forms
cons.table$Manufacturer <- forcats::fct_recode(
  cons.table$Manufacturer, 
  IDT = 'idt', 
  BS = 'bs',
  SA = 'sigma',
  EF = 'eurofins'
)

# Rename "des" variables to match the main naming
cons.table$Purification <- forcats::fct_recode(
  cons.table$Purification, 
  desalted = 'des'
)

# Change from "fwd"/"Rev" inset names to "Insert_1" and "Insert_2"
cons.table$InsertType <- forcats::fct_recode(
  cons.table$InsertType, 
  Insert_1 = 'fwd', 
  Insert_2 = 'rev'
)
```

### Remove reference allele counts

Filter positions based on the reference sequence for this sample. This is meant 
to exclude primer sequences and samples with a different reference sequence. 

At the moment this will not work for samples containing multiple different
primers pairs or samples with variable insert length. That is, all samples need
to:

- have the same insert length
- have the same length forward primer (e.g. 19 nt)

```{r}
cons.table.filtered <- cons.table %>%
  dplyr::filter(
    InsertType %in% c('Insert_1', 'Insert_2'), # Exclude genomic reference
    Position >= 19,        # consider only sequences after forward primer
    Position <= 80         # consider only sequenced before reverse primer
  )

cons.table.filtered <- cons.table

print(dim(cons.table))
print(dim(cons.table.filtered))
```

Set all reference allele counts to 0, so that only *non-reference* counts 
remain for each position. 

```{r}
# Set reference counts to 0
# If processing many samples at once many samples, for the for loop will be slow
for(j in 1:nrow(cons.table.filtered)) {
  row <- cons.table.filtered[j,]
  if( row$Reference == 'A' ) {
    cons.table.filtered[j,]$A <- 0
  }
  else if( row$Reference == 'C' ) {
    cons.table.filtered[j,]$C <- 0
  }
  else if( row$Reference == 'G' ) {
    cons.table.filtered[j,]$G <- 0
  }
  else if( row$Reference == 'T' ) {
    cons.table.filtered[j,]$T <- 0
  }
}
```

## Mean deletions & substitutions error

Calculate the mean deletion and substitution error grouped by Manufacturer,
InsertType, Purification, Batch and Consensus group size.

```{r}
print(
  head(cons.table.filtered)
)

del.count <- cons.table.filtered %>% 
  group_by(Manufacturer, InsertType, Purification, Batch, `Consensus group size`) %>% 
  summarise(del_error = 100*mean(D)/mean(Coverage))

print(
  head(del.count)
)

subs.count <- cons.table.filtered %>% 
  group_by(Manufacturer, InsertType, Purification, Batch, `Consensus group size`) %>% 
  summarise(subs_error = 100*mean(A+C+G+T)/mean(Coverage))

print(
  head(subs.count)
)
```

## SiMSen-Seq error correction (raw versus consensus reads)

We will generate an amplicon plot for each type of Insert, as the the reference
position is used on the x-axis it makes sense to mix multiple different reference 
sequences in the same plot. 

Alternatively, plot using facet_wrap (code not written).

### Insert variant 1

```{r, message=FALSE}
#----------------- // Insert 1 //---------------------
# Get data for insert type "Insert_1" and remove control samples (=cell line)
amplicon.plot.data1 <- cons.table.filtered %>%
    dplyr::mutate(
      total_error = 100*(A+C+G+T+D+I)/Coverage
    ) %>%
    dplyr::filter(Manufacturer != "control", InsertType == "Insert_1") %>%
    tidyr::unite('Type', Manufacturer, Purification, sep = ' ') %>%
    dplyr::group_by(Position, InsertType, Reference, `Consensus group size`) %>%
    dplyr::summarise(
      total_error = mean(total_error)
    ) %>% 
  tidyr::gather("error_type", "error", -c("Position", "InsertType", "Reference", `Consensus group size`))

# Get x-axis labels
labels1 <- amplicon.plot.data1 %>%
  dplyr::ungroup() %>%
  dplyr::distinct(Position, Reference)

# Generate plot for Insert_1
error_plot_1 <- ggplot(
  data = amplicon.plot.data1, 
  mapping = aes(
    x = as.factor(as.character(Position)),
    y = error,
    fill = as.factor(`Consensus group size`),
    color = as.factor(`Consensus group size`))
  ) +
  geom_bar(stat = 'identity', position = 'dodge', width = 0.75) +
  scale_x_discrete(breaks=labels1$Position, labels=labels1$Reference) +
  theme_classic() +
  theme(
    legend.position = 'bottom'
  ) +
  facet_wrap(InsertType ~ ., scales = 'free_x', nrow = 1)

# Draw plot
error_plot_1

# Save plot to pdf
#ggsave(filename = 'Figure_1c_insert_1.pdf', plot = error_plot_1, device = 'pdf')
```

### Insert variant 2

```{r, message=FALSE}
#----------------- // Insert 2 //---------------------
# Get data for insert type "Insert_2" and remove control samples (=cell line)
amplicon.plot.data2 <- cons.table.filtered %>%
    dplyr::mutate(
      total_error = 100*(A+C+G+T+D+I)/Coverage
    ) %>%
    dplyr::filter(Manufacturer != "control", InsertType == "Insert_2") %>%
    tidyr::unite('Type', Manufacturer, Purification, sep = ' ') %>%
    dplyr::group_by(Position, InsertType, Reference, `Consensus group size`) %>%
    dplyr::summarise(
      total_error = mean(total_error)
    ) %>% 
  tidyr::gather("error_type", "error", -c("Position", "InsertType", "Reference", `Consensus group size`))

# Get x-axis labels
labels2 <- amplicon.plot.data2 %>%
  dplyr::ungroup() %>%
  dplyr::distinct(Position, Reference)

# Generate plot for Insert_2
error_plot_2 <- ggplot(
  data = amplicon.plot.data2, 
  mapping = aes(
    x = Position,
    y = error,
    fill = as.factor(`Consensus group size`),
    color = as.factor(`Consensus group size`))
  ) +
  geom_bar(stat = 'identity', position = 'dodge', width = 0.75) +
  scale_x_continuous(breaks=labels2$Position, labels=labels2$Reference) +
  theme_classic() +
  theme(
    legend.position = 'bottom'
  ) +
  facet_wrap(InsertType ~ ., scales = 'free_x', nrow = 1) + 
  scale_fill_brewer(palette = "Pastel1")

# Draw plot
error_plot_2

# Save plot to pdf
#ggsave(filename = 'Figure_1c_insert_2.pdf', plot = error_plot_2, device = 'pdf')
```

## Quantifying deletions in consensus reads

We load the consensus reads and meta data for each sample and then count the
number of deletions in each consensus read. For each sample we then determine
the fraction of reads containing 1 to 7 or more deleted bases. Note, that the 
deletions do not have to be consecutive, but can be spread throughout the read.

First, we load the bam file containing the consensus reads generated by 
UMIErrorCorrect. The following information is then extracted:

- UMI/barcode
- UMI family size

**NOTE!** Processing many alignment files will take a considerable amount of time!
Expect roughly 2 min per sample. 

This code section will be skipped if output files already exist.

```{r processBam, message=FALSE, eval=TRUE}
#---------------------// Reference sequences //---------------------------------
primer_1 ="GTGGTGAGGCTCCCCTTT"
primer_2 = "CAAAGCTGTTCCGTCCCAGT"

forward_insert = "ATACAGAATATCTGTTCGCACTCCGAGTGCGGCTTGCGGAGATTCTCTTCCTCTGTGCGCCG"
reverse_insert = "CTTGCGGAGATTCTCTTCCTCTGTGCGCCGATACAGAATATCTGTTCGCACTCCGAGTGCGG"
#-------------------------------------------------------------------------------

# Because execution of this code block can take some time, check if the the  output
# from this block (df and df2) already exist, if so, skip this block.
df_exists <- file.exists(paste(main,'data/df.csv', sep = ''))
df2_exists <- file.exists(paste(main,'data/df2.csv', sep = ''))

if(!(df_exists && df2_exists)){
  
  # Initialize output tables
  df = data.frame()
  df2 = data.frame()
  
  #How many samples were found?
  print(paste("Found ", nrow(sampleNames), " samples.", sep = ''))
  
  
  
  for(i in 1:nrow(sampleNames)){
    
    print(paste("Processing sample ", i, " out of " , nrow(sampleNames), ".", sep = ''))
    
    out.file <- ''
    
    row <- sampleNames[i,]
    row2 <- sampleNames[i,]
    dir <- sampleNames$File[i]
    
    # Get file(s) ending with '.consensus_reads.bam$', there should only be one!
    bamPath <- list.files(
      path = dir, 
      pattern = '.consensus_reads.bam$',
      full.names = TRUE
    )
    
    # Load consensus reads bam file.
    # TODO This will break if multiple files are found in the step above
    bamFile <- Rsamtools::BamFile(file = bamPath)
    
    # Import binary ‘BAM’ files into a list structure
    alignment <- Rsamtools::scanBam(bamFile)[[1]]
    length <- length(alignment$qname)
    
    # Initialize empty results data frame 
    file = data.frame(
      position=character(), 
      barcode=character(), 
      counts=numeric(), 
      sequence=character(),
      cigar=character()
      )
    
    # Iterate through bam file. This can be very slow!
    for(i in 1:length){
      # get chromosome coordinates
      pos <- alignment$pos[i]
      chrom <- alignment$rname[i]
      coord <- paste(chrom,':',pos,sep='')
      
      # get read sequence
      sequence <- as.character(alignment$seq[i])
      
      # get header containing barcodes and counts
      header <- alignment$qname[i]
      
      # split header
      header <- str_split(header, "_")
      
      # extract barcode from header
      barcode <- header[[1]][4]
      
      # get CIGAR string
      cigar <- alignment$cigar[i]
      
      # extract UMI family size from header and convert to numeric
      count <- header[[1]][5]
      if(!count %in% letters[seq( from = 1, to = 20 )]){
        # TODO this checks if a read is split into multiple chunks labeled a,b,c
        # e.g. for paired-end data, these reads are ignored for now!!
        count <- as.numeric(gsub(".*=","",count))
      
        # save only UMI families with >= threshold reads
        if(count >= consensus_cutoff){
          nrow <- nrow(file)
          file[nrow+1,] <- c(coord, barcode, count, sequence, cigar)
        }
      }
    }
    
    # Get length of read
    file$nchar <- nchar(file$sequence)
    
    # This returns the total number of deleted bases for that string
    # Examples:
    # ACTG-DD-AT gives 2
    # ATG-D gives 1
    # A-DDD-TCGA-DD-TGC gives 5
    file$n_D <- processCigar(file$cigar)
    
    # For each sample calculate the fraction of reads that contain 0,1,2,...,6 or
    # 7 or more deleted bases
    nreads <- nrow(file)
    
    row$`0` <- 100*(nrow(file[file$n_D == 0,])/nreads)
    row$`1` <- 100*(nrow(file[file$n_D == 1,])/nreads)
    row$`2` <- 100*(nrow(file[file$n_D == 2,])/nreads)
    row$`3` <- 100*(nrow(file[file$n_D == 3,])/nreads)
    row$`4` <- 100*(nrow(file[file$n_D == 4,])/nreads)
    row$`5` <- 100*(nrow(file[file$n_D == 5,])/nreads)
    row$`6` <- 100*(nrow(file[file$n_D == 6,])/nreads)
    row$`>7` <- 100*(nrow(file[file$n_D >= 7,])/nreads)
    
    # Here we compute the number of times that 2,3,4 or 5 bases were deleted
    # consecutively in each read, i.e. ACT-DD-ACT counts, but AC-D-AT-D-CT does
    # not even though both have 2 deleted bases.
    # TODO this does not take into account other situations, such as 6 consecutively deleted bases
    file$n_D2 <- stringr::str_count(string = file$cigar, pattern = '2D')
    file$n_D3 <- stringr::str_count(string = file$cigar, pattern = '3D')
    file$n_D4 <- stringr::str_count(string = file$cigar, pattern = '4D')
    file$n_D5 <- stringr::str_count(string = file$cigar, pattern = '5D')
      
    # Count reads with exactly 2 deleted bases, where _D == 2, irrespective of
    # whether they occurred together DD or separately ACT-D-ACT-D-ACT, both are n_D = 2.
    row2$n_two_errors <- nrow(file %>% dplyr::filter(n_D == 2))
    
    # Count reads where exactly 2 bases were deleted AND these occurred consecutively
    row2$n_two_errors_together <- nrow(file %>% dplyr::filter(n_D == 2, n_D2 == 1))
      
    # Repeat for 3 bases
    row2$n_three_errors <- nrow(file %>% dplyr::filter(n_D == 3))
    row2$n_three_errors_together <- nrow(file %>% dplyr::filter(n_D == 3, n_D3 == 1))
      
    # Repeat for 4 bases
    row2$n_four_errors <- nrow(file %>% dplyr::filter(n_D == 4))
    row2$n_four_errors_together <- nrow(file %>% dplyr::filter(n_D == 4, n_D4 == 1))
    
    df <- rbind(df,row)
    df2 <- rbind(df2,row2)
  }
  write_csv(x = df, file = 'data/df.csv')
  write_csv(x = df2, file = 'data/df2.csv')
} else{
  # Else, if output exists, import them 
  df = readr::read_csv(paste(main,'data/df.csv', sep = ''))
  df2 = readr::read_csv(paste(main,'data/df2.csv', sep = ''))
}
```

Changing variable names as before. Can be omitted using eval=FALSE if needed.

```{r, message=FALSE, eval=TRUE}
# Change names of factor variables
df$Manufacturer <- forcats::fct_recode(
  df$Manufacturer, 
  IDT = 'idt', 
  BS = 'bs',
  SA = 'sig',
  EF = 'eurofins'
)

df$Purification <- forcats::fct_recode(
  df$Purification, 
  desalted = 'des'
)

df$InsertType <- forcats::fct_recode(
  df$InsertType, 
  Insert_1 = 'fwd', 
  Insert_2 = 'rev'
)

#write.csv(
#  x = df, 
#  file = paste(Sys.Date(),'consensus_read_deletions_counts.csv', sep = '_')
#)

# Change names of factor variables
df2$Manufacturer <- forcats::fct_recode(
  df2$Manufacturer, 
  IDT = 'idt', 
  BS = 'bs',
  SA = 'sig',
  EF = 'eurofins'
)

df2$Purification <- forcats::fct_recode(
  df2$Purification, 
  desalted = 'des'
)

df2$InsertType <- forcats::fct_recode(
  df2$InsertType, 
  Insert_1 = 'fwd', 
  Insert_2 = 'rev'
)
```

### Deletion boxplots

If multiple bases are deleted, calculated the fraction of deletions that occurred together.
For example data_2 calculates the situation for 2 deleted bases:
If n_two_errors is non zero there are 2 deleted bases in that read, then calculate the
fraction n_two_errors_together/n_two_errors, otherwise the value is 0. The same 
calculation is performed for 3 and 4 deletions, respectively.

```{r}
data_2 <- df2 %>% 
  dplyr::mutate(ratio = if_else(n_two_errors != 0, n_two_errors_together/n_two_errors, 0)) %>% 
  tidyr::unite("Type", Manufacturer, Purification, InsertType, sep = " ") %>%
  dplyr::group_by(Type) %>%
  dplyr::summarise(mean = mean(ratio), sd = sd(ratio))


data_3 <- df2 %>% 
  dplyr::mutate(ratio = if_else(n_three_errors != 0, n_three_errors_together/n_three_errors, 0)) %>% 
  tidyr::unite("Type", Manufacturer, Purification, InsertType, sep = " ") %>%
  dplyr::group_by(Type) %>%
  dplyr::summarise(mean = mean(ratio), sd = sd(ratio))


data_4 <- df2 %>% 
  dplyr::mutate(ratio = if_else(n_four_errors != 0, n_four_errors_together/n_four_errors, 0)) %>% 
  tidyr::unite("Type", Manufacturer, Purification, InsertType, sep = " ") %>%
  dplyr::group_by(Type) %>%
  dplyr::summarise(mean = mean(ratio), sd = sd(ratio)) 


data_2["Del"] <- "2"
data_3["Del"] <- "3"
data_4["Del"] <- "4"

data <- rbind(data_2,data_3,data_4)
```

Now we plot this data as boxplots

```{r}
boxplot_deletions <- ggplot2::ggplot(
  data = data, 
  mapping = aes(
    x = as.factor(Del), 
    y = 100*(mean)
    )
  ) +
  geom_boxplot(colour = "grey50") +
  geom_jitter(
      width = 0.1
    ) +
  xlab(label = "Number of Deletions") +
  ylab(label = "At least two consecutive deletions (%)") +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 12,colour = "black"),
    axis.text = element_text(size = 10,colour = "black"),
    axis.ticks = element_line(size = 1),
    panel.border = element_blank(), 
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.line = element_line(colour = 'black'),
    legend.position = "bottom"
    )

# add comment  
boxplot_deletions

#ggsave(filename = 'boxplot_deletions.pdf', plot = boxplot_deletions, device = 'pdf')
```

We can also plot deletions as a barplot

```{r}
df3 <- df %>% 
  tidyr::unite("Type", Manufacturer, Purification, InsertType, sep = " ") %>%
  dplyr::group_by(Type) %>%
  dplyr::summarise(mean = mean(`0`))


order <- df3[order(df3$mean, decreasing = TRUE),]$Type

data$Type <- as.factor(data$Type)
data$Type <- forcats::fct_relevel(data$Type, c('IDT gblocks Insert_1','IDT gblocks Insert_2', 'control MCF7 Insert_1'), after = Inf)
#data$Type <- fct_relevel(data$Type, order)

barplot_deletions <- ggplot2::ggplot(
  data = data, 
  mapping = aes(
    x = Type, 
    y = 100*mean
    )
  ) +
  facet_wrap(Del ~ ., ncol = 1, strip.position="right") +
  geom_bar(stat = "identity") +
  geom_hline(yintercept = 0) + 
  theme_minimal() +
  theme(
    axis.title = element_text(size = 12,colour = "black"),
    axis.text = element_text(size = 10,colour = "black"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    axis.ticks = element_line(size = 1),
    panel.border = element_blank(), 
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.line = element_line(colour = 'black'),
    legend.position = "bottom"
    )


barplot_deletions

#ggsave(filename = 'barplot_deletions.pdf', plot = barplot_deletions, device = 'pdf')
```

### Figure 1b

Now we plot the fraction of deletions in consensus 10 reads for the two insert 
variants and all 3 batches side-by-side.

```{r}
plot_data <- df %>% 
  tidyr::gather(
    key = 'Deletions', 
    value = 'Fraction of reads', 
    -c(Manufacturer,InsertType,Purification,Batch,Replicate,File)
  ) %>%
  dplyr::filter(InsertType != 'gDNA') %>%
  tidyr::unite(col = 'Type', Manufacturer, Purification, sep = ' ') %>%
  dplyr::group_by(Type, InsertType, Deletions) %>%
  dplyr::summarise(`Fraction of reads` = mean(`Fraction of reads`))


plot_data$Deletions <- forcats::fct_relevel(plot_data$Deletions, '>7', after = Inf)
plot_data$Type <- forcats::fct_relevel(plot_data$Type, c('IDT gblocks', 'control MCF7'), after = Inf)

# Custom colour palette: blue -> green -> red
plot_colours <- c('#11122F', '#3462AB', '#5EA4D1', '#92D1DA', '#B9DDD5', '#F7C75D', '#E3985B', '#D31D00')

figure1b <- ggplot2::ggplot(
  data = plot_data, 
  mapping = aes(
    x = Type, 
    y = `Fraction of reads`, 
    fill = Deletions
    )
  ) +
  geom_bar(position=position_stack(reverse = TRUE) , stat = 'identity') +
  facet_wrap(InsertType~., scales = 'free_x', strip.position = 'right') +
  theme_minimal() +
  theme(
    axis.text.x = element_text(
      hjust = 1,
      angle = 45,
      size = 9
    ),
    panel.border = element_blank(), 
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.line = element_line(colour = 'black'),
    legend.position = "bottom"
    ) +
  #scale_fill_brewer(palette = 'Spectral',direction = -1) +
  scale_fill_manual(values = plot_colours) +
  coord_cartesian(ylim = c(85,NA))

figure1b
  
#ggsave(filename = 'Figure_1b.pdf', plot = figure1b, device = 'pdf')
```

## Wilcoxon tests

### Insert Type 1

```{r wilcoxon, echo=FALSE}
# Filter and transform data
cons.variants <- cons.table.filtered %>%
      dplyr::mutate(
        subsitution_error = 100*(A+C+G+T)/Coverage,
        indel_error = 100*(D+I)/Coverage,
        total_error =  100*(A+C+G+T+D+I)/Coverage
      ) %>%
      dplyr::filter(
        `Consensus group size` == consensus_cutoff,
        InsertType == 'Insert_1',
        Batch %in% c("b1", "b2", "b3"),
        Manufacturer != "control"
      ) %>%
      tidyr::unite('Type', Manufacturer, Purification, InsertType, Batch, sep = ' ') %>%
      dplyr::group_by(Type) %>%
  dplyr::select(Position, Type, indel_error)

wilcox_results = stats::pairwise.wilcox.test(
  x = cons.variants$indel_error, 
  g = cons.variants$Type, 
  exact = FALSE, 
  paired = FALSE, 
  p.adjust.method = "bonferroni"
)

#write.table(
#  x = wilcox_results$p.value, file = "wilcoxon_test_results_insert_1.csv",
#  sep = ",", row.names = TRUE
#  )

wilcox_results$p.value 
```

### Insert Type 2

```{r, echo=FALSE}
cons.variants <- cons.table.filtered %>%
      dplyr::mutate(
        subsitution_error = 100*(A+C+G+T)/Coverage,
        indel_error = 100*(D+I)/Coverage,
        total_error =  100*(A+C+G+T+D+I)/Coverage
      ) %>%
      dplyr::filter(
        `Consensus group size` == consensus_cutoff,
        InsertType == 'Insert_2',
        Batch %in% c("b1", "b2", "b3"),
        Manufacturer != "control"
      ) %>%
      tidyr::unite('Type', Manufacturer, Purification, InsertType, Batch, sep = ' ') %>%
      dplyr::group_by(Type) %>%
  dplyr::select(Position, Type, indel_error)

wilcox_results = stats::pairwise.wilcox.test(
  x = cons.variants$indel_error, 
  g = cons.variants$Type, 
  exact = FALSE, 
  paired = FALSE, 
  p.adjust.method = 'bonferroni'
)

#write.table(
#  x = wilcox_results$p.value, file = "wilcoxon_test_results_insert_2.csv",
#  sep = ",", row.names = TRUE
#  )

wilcox_results$p.value 
```

## Dinucleotide combinations

```{r dinucleotides, echo=FALSE}
# Retrieve consensus data for Insert_1 and the diucleotide positions only (19 - 50)
cons.variants <- cons.table.filtered %>%
      dplyr::mutate(
        subsitution_error = 100*(A+C+G+T)/Coverage,
        indel_error = 100*(D+I)/Coverage,
        total_error =  100*(A+C+G+T+D+I)/Coverage
      ) %>%
      dplyr::filter(
        `Consensus group size` == consensus_cutoff,
        InsertType == 'Insert_1',
        Batch == c("b1", "b2", "b3"),
        Manufacturer != "control",
        Position >= 19,
        Position <= 50
      ) %>%
      tidyr::unite('Type', Manufacturer, Purification, InsertType, Batch, sep = ' ') %>%
      dplyr::group_by(Type) %>%
  dplyr::select(Position, Reference, Type, indel_error) %>%
  dplyr::arrange(Position, .by_group = TRUE)

# Initialize new data frame
A = data.frame()


for( i in 1:nrow(cons.variants) ){
  
  row <- cons.variants[i,]
  
  position <- cons.variants$Position[i]
  next_base <- cons.variants$Reference[i+1]
  
  if(position < 50){
    # 
    row$next_base <- next_base
  } else {
    # If reaching the end of dinucleotide sequence, the next base is C
    row$next_base <- 'C'
  }
  
  A <- rbind(A,row)

}

base_plot_data <- A %>% ungroup() %>% group_by(next_base) %>% 
  summarise(
    mean_error = mean(indel_error),
    sd_error = sd(indel_error)/sqrt(n())
    )

A_error <- A[A$next_base == 'A',]$indel_error
C_error <- A[A$next_base == 'C',]$indel_error
T_error <- A[A$next_base == 'T',]$indel_error
G_error <- A[A$next_base == 'G',]$indel_error

t.test(x = A_error, y = C_error)
t.test(x = A_error, y = G_error)
t.test(x = T_error, y = G_error)
t.test(x = T_error, y = C_error)
t.test(x = C_error, y = G_error)

base_plot <- ggplot(
  data = base_plot_data,
  mapping = aes(
    x=next_base,
    y=mean_error,
    fill=next_base
    )
  ) +
  geom_bar(stat = "identity") +
  scale_fill_brewer(palette = "Set1") +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 16,colour = "black"),
    axis.text = element_text(size = 14,colour = "black"),
    axis.ticks = element_line(size = 1),
    panel.border = element_blank(), 
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.line = element_line(colour = 'black'),
    legend.position = "bottom"
    ) +
  xlab("Base") +
  ylab("Deletion error (%)") +
  geom_errorbar(
   mapping = aes(ymin = mean_error-sd_error, ymax = mean_error+sd_error), 
    position = position_dodge(
      width = 0.6, 
      preserve = "single"
    ), 
    width = 0.25
    ) 

base_plot

#ggsave(filename = 'base_plot_insert_2.pdf', plot = base_plot, device = 'pdf', width = 6, height = 6)
```

```{r, echo=FALSE}
data_to_plot <- cons.variants %>% 
  ungroup() %>% 
  group_by(Position, Reference) %>% 
  summarise(error = indel_error - mean(indel_error))


dinuc_plot <- ggplot2::ggplot(
  data = cons.variants, 
  mapping = aes(
    x = as.factor(Position), 
    y = indel_error,
    fill = Reference
    )
  ) +
  geom_boxplot() +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 12,colour = "black"),
    axis.text = element_text(size = 10,colour = "black"),
    axis.ticks = element_line(size = 1),
    panel.border = element_blank(), 
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.line = element_line(colour = 'black'),
    legend.position = "bottom"
    ) +
  scale_fill_brewer(palette = "Set1") +
  xlab("Nucleotide position") +
  ylab("Deletion error (%)")

dinuc_plot

#ggsave(filename = 'dinuc_plot_insert2_.pdf', plot = dinuc_plot, device = 'pdf', width = 12, height = 8)
```


```{r, echo=FALSE, eval=FALSE}
cons.variants_1 <- cons.table.filtered %>%
      dplyr::mutate(
        subsitution_error = 100*(A+C+G+T)/Coverage,
        indel_error = 100*(D+I)/Coverage,
        total_error =  100*(A+C+G+T+D+I)/Coverage
      ) %>%
      dplyr::filter(
        `Consensus group size` == consensus_cutoff,
        InsertType == 'Insert_1',
        Batch == c("b1", "b2", "b3"),
        Manufacturer != "control",
        Position >= 19,
        Position <= 50
      ) %>%
      tidyr::unite('Type', Manufacturer, Purification, InsertType, Batch, sep = ' ') %>%
      dplyr::group_by(Type) %>%
  dplyr::select(Position, Reference, Type, indel_error) %>%
  dplyr::arrange(Position, .by_group = TRUE)

cons.variants_2 <- cons.table.filtered %>%
      dplyr::mutate(
        subsitution_error = 100*(A+C+G+T)/Coverage,
        indel_error = 100*(D+I)/Coverage,
        total_error =  100*(A+C+G+T+D+I)/Coverage
      ) %>%
      dplyr::filter(
        `Consensus group size` == consensus_cutoff,
        InsertType == 'Insert_2',
        Batch == c("b1", "b2", "b3"),
        Manufacturer != "control",
        Position >= 49,
        Position <= 80
      ) %>%
      tidyr::unite('Type', Manufacturer, Purification, InsertType, Batch, sep = ' ') %>%
      dplyr::group_by(Type) %>%
  dplyr::select(Position, Reference, Type, indel_error) %>%
  dplyr::arrange(Position, .by_group = TRUE)


a_1 <- c(25, 33, 41, 49)
b_1 <- c(26, 34, 42, 50)

a_2 <- c(55, 63, 71, 79)
b_2 <- c(56, 64, 72, 80)

c <- cons.variants_1 %>% 
  group_by(Position, Reference) %>% 
  summarise(mean=median(indel_error)) %>% 
  filter(Position %in% a_1)

d <- cons.variants_1 %>% 
  group_by(Position, Reference) %>% 
  summarise(mean=median(indel_error)) %>% 
  filter(Position %in% b_1)

e <- cons.variants_2 %>% 
  group_by(Position, Reference) %>% 
  summarise(mean=median(indel_error)) %>% 
  filter(Position %in% a_2)

f <- cons.variants_2 %>% 
  group_by(Position, Reference) %>% 
  summarise(mean=median(indel_error)) %>% 
  filter(Position %in% b_2)


wilcox.test(c$mean, d$mean, paired = TRUE, alternative = "two.sided")

wilcox.test(e$mean, f$mean, paired = TRUE, alternative = "two.sided")

before <- c(c$mean, e$mean)
after <- c(d$mean, f$mean)

insert <- c("Sequence variant 1","Sequence variant 1","Sequence variant 1","Sequence variant 1","Sequence variant 2","Sequence variant 2","Sequence variant 2","Sequence variant 2")

base <- c("A", "C", "T", "G","A", "C", "T", "G","A", "C", "T", "G","A", "C", "T", "G")

df <- data.frame(before = before, after = after, insert = insert, base = base) 
df <- df %>% tidyr::gather("group", "value", -c("insert", "base"))

df <- df %>% mutate(group=replace(group, group=="before", "5'")) %>% mutate(group=replace(group, group=="after", "3'"))

plot <- ggplot(df, aes(x=group, y=value, group = base, col = base)) +
    geom_point(size=2) +
    geom_line() +
    xlab("") + ylab("Median deletion error (%)") +
    facet_wrap(~insert) +
    theme_classic(base_size = 13) +
    theme(legend.position="top") + 
    scale_color_brewer(palette = "Set1")

plot

#ggsave(filename = 'sequential_error_plot.pdf', plot = plot, device = 'pdf', width = 8, height = 4)
```

## Correlations

Are deletions and substitutions correlated with each other?

```{r cortest1, echo=FALSE}
# Filter and transform data
cons.variants <- cons.table.filtered %>%
      dplyr::mutate(
        subsitution_error = 100*(A+C+G+T)/Coverage,
        indel_error = 100*(D+I)/Coverage,
        total_error =  100*(A+C+G+T+D+I)/Coverage
      ) %>%
      dplyr::filter(
        `Consensus group size` == consensus_cutoff,
        InsertType == 'Insert_1',
        Batch == 'b1',
        Manufacturer != "control"
      ) %>%
      tidyr::unite('Type', Manufacturer, Purification, sep = ' ') %>%
      dplyr::group_by(Type, Position, InsertType, Batch) %>%
      dplyr::summarise(
        Substitutions = mean(subsitution_error),
        subs_std = sd(subsitution_error),
        Deletions = mean(indel_error),
        indels_std = sd(indel_error),
        Error = mean(total_error)
      ) %>% ungroup()

M <- cons.variants %>% select(Position, Substitutions, Deletions) 
cor.test(x = M$Deletions, y = M$Substitutions, method = "spearman")
```

Is the mean level of substitutions significantly different from SiMSen-Seq background?

```{r cortest2, echo=FALSE}
# Filter and transform data
cons.variants <- cons.table.filtered %>%
      dplyr::mutate(
        subsitution_error = 100*(A+C+G+T)/Coverage,
        indel_error = 100*(D+I)/Coverage,
        total_error =  100*(A+C+G+T+D+I)/Coverage
      ) %>%
      dplyr::filter(
        `Consensus group size` == consensus_cutoff,
        InsertType %in% c('Insert_1', 'Insert_2'),
        Batch  %in% c('b1', 'b2', 'b3'),
        Manufacturer != "control"
      ) %>%
      tidyr::unite('Type', Manufacturer, Purification, sep = ' ') %>%
      dplyr::group_by(Type, Position, InsertType, Batch) %>%
      dplyr::summarise(
        Substitutions = mean(subsitution_error),
        subs_std = sd(subsitution_error),
        Deletions = mean(indel_error),
        indels_std = sd(indel_error),
        Error = mean(total_error)
      ) %>% ungroup()

# Summarize mean substitution error per sample
M <- cons.variants %>% 
  dplyr::group_by(Type, InsertType, Batch) %>%
  dplyr::select(Position, Substitutions) %>% 
  dplyr::summarise(mean = mean(Substitutions))
  
# Compare to known SiMSen-Seq error rate
t.test(M$mean, mu = 0.0005, alternative = "two.sided")
```

### Correlations by batches

```{r, echo=FALSE}

insertType = c('Insert_1', 'Insert_2')
batches = c('b1','b2','b3')

for(insert in insertType){
  for(b in batches){
    
    # Filter and transform data
    cons.variants <- cons.table.filtered %>%
          dplyr::mutate(
            subsitution_error = 100*(A+C+G+T)/Coverage,
            indel_error = 100*(D+I)/Coverage,
            total_error =  100*(A+C+G+T+D+I)/Coverage
          ) %>%
          dplyr::filter(
            `Consensus group size` == consensus_cutoff,
            InsertType == insert,
            Batch == b,
            Manufacturer != 'control'
          ) %>%
          tidyr::unite('Type', Manufacturer, Purification, sep = ' ') %>%
          dplyr::group_by(Type, Position, InsertType, Batch) %>%
          dplyr::summarise(
            Substitutions = mean(subsitution_error),
            subs_std = sd(subsitution_error),
            Deletions = mean(indel_error),
            indels_std = sd(indel_error),
            Error = mean(total_error)
          ) %>% ungroup()
    
    variants <- c('Substitutions', 'Deletions', 'Error')
    
    # Relevel variables for plotting purposes
    # cons.variants$Type <- forcats::fct_relevel(cons.variants$Type, c('IDT gblocks', 'control MCF7'), after = Inf)
    
    for(var in variants){
      
      M <- cons.variants %>% select(Position, Type, !!as.symbol(var)) %>%
        spread(Type, !!as.symbol(var)) %>% select(-Position)
      
      # Compute correlation matrix
      cor <- round(cor(M, method = 'spearman'),1)
      cor[cor>1] <- NA
      
      # Test for association between paired samples
      res1 <- corrplot::cor.mtest(
        mat = M, 
        conf.level = .95, 
        method = 'spearman', 
        exact = FALSE,
        alternative = 'two.sided'
      )
      
      print(paste(var,"_",insert,"_",b,"; ", mean(cor, na.rm = TRUE), sep =""))
      
      pAdj <- p.adjust(c(res1[[1]]), method = "BH")
      resAdj <- matrix(pAdj, ncol = dim(res1[[1]])[1])
      
      colnames(resAdj) <- colnames(M)
      rownames(resAdj) <- colnames(M)
      
      cor_plot <- ggcorrplot::ggcorrplot(
        corr = cor, 
        p.mat = resAdj,
        sig.level = 0.05,
        lab = TRUE, 
        ggtheme = theme_minimal(),
        lab_size = 5, 
        show.diag = FALSE
        ) + 
        scale_fill_gradient2(
          limit = c(
            min(cor, na.rm = TRUE),
            max(cor, na.rm = TRUE)
            ), 
          low = viridis::viridis(100)[1], 
          high =  viridis::viridis(100)[100], 
          mid = viridis::viridis(100)[50], 
          midpoint = mean(c(min(cor, na.rm = TRUE),max(cor, na.rm = TRUE)))
          )
      
      # Show correlation plot
      cor_plot
      
      #write.table(
      #  x = M, 
      #  file = paste("correlation_data_",var,"_",insert,"_",b,".csv", sep = ""), 
      #  sep = ";"
      #  )
      
      #write.table(
      #  x = resAdj, 
      #  file = paste("correlation_adjusted_p_values",var,"_",insert,"_",b,".csv", sep = ""), 
      #  sep = ";"
      #  )
    
      #ggsave(
      #  filename = paste("Figure_X_corrrelation_plot_",var,"_",insert,"_",b,".pdf", sep = ""),
      #  plot = cor_plot, 
      #  device = 'pdf'
      #  )
    }
  }
}
```

### Correlations for all batches together

```{r, echo=FALSE}
cons.variants <- cons.table.filtered %>%
      dplyr::mutate(
            subsitution_error = 100*(A+C+G+T)/Coverage,
            indel_error = 100*(D+I)/Coverage,
            total_error =  100*(A+C+G+T+D+I)/Coverage
      ) %>%
      dplyr::filter(
        `Consensus group size` == consensus_cutoff, # Use consensus error only
        InsertType == 'Insert_1',
        Manufacturer != "control",
        Batch %in% c('b1','b2','b3')
      ) %>%
      tidyr::unite('Type', Manufacturer, Purification, Batch, sep = ' ') %>%
      dplyr::group_by(Position, Type, InsertType) %>%
      dplyr::summarise(
        Substitutions = mean(subsitution_error),
        subs_std = sd(subsitution_error),
        Deletions = mean(indel_error),
        indels_std = sd(indel_error),
        Error = mean(total_error)
      ) %>% ungroup()

# Re-order factor levels for plotting
#cons.variants$Type <- forcats::fct_relevel(cons.variants$Type, c('IDT gblocks b1', 'control MCF7'), after = Inf)
    
mean_errors <- cons.variants %>% 
  group_by(Type) %>% 
  summarise(
    mean_subs = mean(Substitutions), 
    mean_dels = mean(Deletions),
    mean_total = mean(Substitutions+Deletions)
  )


print("Mean deletion error: ")
mean(mean_errors$mean_dels)

print("Mean substitution error: ")
mean(mean_errors$mean_subs)


variants <- c("Substitutions", "Deletions", "Error")

for(var in variants){
  M <- cons.variants %>% 
  select(Position, Type, !!as.symbol(var)) %>%
  spread(Type, !!as.symbol(var)) %>% select(-Position)
      
  # Compute correlation matrix
  cor <- round(cor(M, method = "spearman"),1)
  cor[cor>=1] <- NA
        
  # Test for association between paired samples
  res1 <- corrplot::cor.mtest(
    mat = M, 
    conf.level = .95, 
    method = "spearman", 
    exact = FALSE,
    alternative = "two.sided"
  )
        
  #print(paste(var,"_",insert,"; ", mean(cor, na.rm = TRUE), sep =""))
  
  # Adjust p-values for multiple testing
  pAdj <- p.adjust(c(res1[[1]]), method = "BH")
  resAdj <- matrix(pAdj, ncol = dim(res1[[1]])[1])
        
  colnames(resAdj) <- colnames(M)
  rownames(resAdj) <- colnames(M)
        
  cor_plot <- ggcorrplot::ggcorrplot(
    corr = cor, 
    p.mat = resAdj,
    sig.level = 0.05,
    lab = TRUE, 
    type = 'upper',
    ggtheme = theme_minimal(),
    lab_size = 2, 
    show.diag = FALSE
    ) + 
    scale_fill_gradient2(
      limit = c(
        min(cor, na.rm = TRUE),
        max(cor, na.rm = TRUE)
        ),
      low = viridis::viridis(100)[1], 
      high =  viridis::viridis(100)[100], 
      mid = viridis::viridis(100)[50], 
      midpoint = mean(c(min(cor, na.rm = TRUE),max(cor, na.rm = TRUE)))
    )
  
  cor_plot
        
  #write.table(
  #  x = M,
  #  file = paste("correlation_data_",var,"_",insert,"_all-batches",".csv", sep = ""),
  #  sep = ";"
  #)
        
  #write.table(
  #  x = resAdj,
  #  file = paste("correlation_adjusted_p_values",var,"_",insert,"_all-batches",".csv", sep = ""),
  #  sep = ";"
  #)
      
  #ggplot2::ggsave(
  #  filename = paste("Figure_X_corrrelation_plot_",var,"_",insert,"_all-batches",".pdf", sep = ""),
  #  plot = cor_plot,
  #  device = 'pdf'
  #)
}

```

## Figure 2

```{r perBaseErrorPlot, echo=FALSE}
figure_2 <- perBaseErrorPlot(
  data = cons.table.filtered,
  insertType = c('Insert_1', 'gDNA'),
  batch = c('b1')
)

figure_2

#ggsave(filename = 'Figure_2_v2.pdf', plot = figure_2, device = 'pdf')
```

### Insert variant 2

```{r}
cons.variants <- cons.table.filtered %>%
  dplyr::mutate(
      subsitution_error = 100*(A+C+G+T)/Coverage,
      indel_error = 100*(D+I)/Coverage,
      total_error =  100*(A+C+G+T+D+I)/Coverage
  ) %>%
  dplyr::filter(
    `Consensus group size` == consensus_cutoff,
    Manufacturer %in% c('IDT', 'SA'),
    Purification %in% c('desalted', 'PAGE'),
    InsertType %in% c('Insert_1'),
    Batch %in% c('b1','b2', 'b3'),
    Position >= 19, 
    Position <= 80
    ) %>%
  dplyr::group_by(Position, Manufacturer,Purification, InsertType, Batch, Reference) %>%
  dplyr::summarise(
    `Consensus 10 error (%)` = mean(total_error),
    sd = sd(total_error)
  )

df <- cons.variants %>% ungroup() %>% 
  group_by(Manufacturer,Purification,InsertType,Batch) %>%
  summarise(
    mean_error = mean(`Consensus 10 error (%)` ),
    median_error = median(`Consensus 10 error (%)` )
    )
  
plot <- ggplot(
  data = cons.variants, 
  mapping = aes(x=Batch,y=`Consensus 10 error (%)` , fill = Batch)
  ) +
  geom_boxplot() + theme_classic() + 
  scale_fill_manual(values=c("gray55", "#f6be4c", "#469ed4")) +
  scale_color_manual(values=c("gray55", "#f6be4c", "#469ed4")) + 
  theme(
    axis.text.x = element_text(size = 6), 
    panel.border = element_blank(), 
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.line = element_line(colour = "black"),
    axis.ticks.x = element_line(colour = "black"),
    axis.ticks.y = element_line(colour = "black")
    ) +
  geom_hline(
    yintercept = 0,
    colour="gray55",
    linetype="dashed"
  ) 

plot

#df2 <- cons.variants %>% ungroup() %>% tidyr::unite('Type', Manufacturer, Purification, sep = ' ') %>%
#  dplyr::select(Position, Type,Batch, cons_error)

#res.aov2 <- aov(cons_error ~ Type + Batch, data = df2)
#summary(res.aov2)

#TukeyHSD(res.aov2, which = "Type")
#TukeyHSD(res.aov2, which = "Batch")
```

## Figure 3 barplot

```{r}

cons.variants <- cons.table.filtered %>%
  dplyr::mutate(
      subsitution_error = 100*(A+C+G+T)/Coverage,
      indel_error = 100*(D+I)/Coverage,
      total_error =  100*(A+C+G+T+D+I)/Coverage
  ) %>%
  dplyr::filter(
    `Consensus group size` == consensus_cutoff,
    Manufacturer %in% c('IDT', 'SA'),
    Purification %in% c('desalted', 'PAGE'),
    InsertType %in% c('Insert_1'),
    Position >= 19, 
    Position <= 80
    ) %>%
  dplyr::group_by(Position, Manufacturer,Purification, InsertType, Batch, Reference) %>%
  dplyr::summarise(
    `Consensus 10 error (%)` = mean(total_error),
    sd = sd(total_error)
  )

consvar <- cons.variants %>% ungroup() %>%
  dplyr::mutate(Type = paste(Manufacturer,Purification, sep = '_')) %>%
  group_by(Type, Batch) %>%
  dplyr::mutate(`Difference to mean error` = `Consensus 10 error (%)` - mean(`Consensus 10 error (%)`))


# Create plot object
plot <- ggplot(
  data = consvar, 
  mapping = aes(
    x = Position,
    y = `Difference to mean error`,
    fill = Batch
    )
  ) + 
  geom_bar(mapping = aes(col=Batch),position="dodge", stat="identity", colour="black") + 
  facet_grid(Type ~ Batch,scales = 'free_x') +
  theme_minimal() + 
  scale_fill_manual(values=c("gray55", "#f6be4c", "#469ed4")) +
  scale_color_manual(values=c("gray55", "#f6be4c", "#469ed4")) + 
  theme(
    axis.text.x = element_text(size = 9), 
    panel.border = element_blank(), 
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.line = element_line(colour = "black"),
    axis.ticks.x = element_line(colour = "black"),
    axis.ticks.y = element_line(colour = "black")
    ) +  coord_cartesian(ylim = c(NA, 1.5)) +
  geom_hline(
    yintercept = 0
  ) 

plot

#ggsave(
#  plot = plot,
#  filename = 'Figure_2_insert_2_alternative_v1.pdf', 
#  device = 'pdf', width = 8, height = 6
#)
```

#### Outlier testing

```{r, eval=FALSE}
# TODO This is not used anywhere.
compute_correlation <- function(data,batch){
  corr <- data %>% ungroup() %>%
  dplyr::mutate(Type = paste(Manufacturer,Purification, sep = '_')) %>%
  group_by(Type) %>%
  select(Position, `Consensus 10 error (%)`) %>%
  filter(Type == batch) %>%
  tidyr::spread(Batch, `Consensus 10 error (%)`)

  corr <- as.data.frame(corr)
  rownames(corr) <- corr$Position
  corr$Type <- NULL
  corr$Position <- NULL
  
  r <- Hmisc::rcorr(as.matrix(corr), type="spearman")
  
  return(r)
}

df <- cons.variants %>% ungroup() %>%
  dplyr::mutate(Type = paste(Manufacturer,Purification, sep = '_')) %>%
  group_by(Type, Batch) %>%
  select(Position, `Consensus 10 error (%)`) %>%
  filter(Type == "IDT_desalted") %>%
  tidyr::spread(Batch, `Consensus 10 error (%)`)

# Perform outlier test on batch 'b1'
# TODO loop over all batches
test <- EnvStats::rosnerTest(df$b1,k = 5)

# Index for outliers in the original data set
outliers <- test$all.stats[test$all.stats$Outlier == TRUE,]$Obs.Num
```

## Ribbon plot

### Insert variant 1

**Insert variant 1.** b1, b2, b3 denote different batches. Lines show the mean error 
with ribbons indicating standard deviation (n = 3 technical replicates).

```{r ribbonPlot1}
fig <- ribbonPlot(
  data = cons.table.filtered,
  manufacturers = c("IDT", "SA"),
  purifications = c("desalted", "PAGE"),
  insertTypes = c("Insert_2"),
  start_pos = 19,
  end_pos = 80
)

fig

#ggsave(
#  plot = fig,
#  filename = 'Figure_3.pdf', 
#  device = 'pdf'
#)
```

### Insert variant 2

**Insert variant 2.** b1, b2, b3 denote different batches. Lines show the mean error 
with ribbons indicating standard deviation (n = 3 technical replicates).

```{r ribbonPlot2}

fig <- ribbonPlot(
  data = cons.table.filtered,
  manufacturers = c("IDT", "SA"),
  purifications = c("desalted", "PAGE"),
  insertTypes = c("Insert_2"),
  start_pos = 19,
  end_pos = 80
)

fig

#ggsave(
#  plot = amplicon_plot,
#  filename = 'Figure_4.pdf',
#  device = 'pdf'
#)
```

## Supplemental Figures

### Supplemental Fig. 1a - qPCR data

A DNA dilution series was analyzed using qPCR of the target primers of the assay used
in this study. Here we plot the mean Cq values and confidence intervals.

```{r, warning=FALSE}
# Import data
cq_values <- read.table(file = "data/cq_tp1_dilution_series.csv", sep = ",", header = TRUE)

# Reformat data for plotting 
cq_values <- cq_values %>% 
  tidyr::gather("Replicate", "Cq", -c("Type", "Input")) %>% 
  dplyr::filter(Input != 0, Type == "MCF-7")

# Define experimental design
my.formula <- y ~ x

# Generate plot
cq_plot <- ggplot2::ggplot(
  data = cq_values, 
  mapping = aes(
    x = log(Input,10), 
    y = Cq
  )) + 
  ggplot2::geom_point(
    aes(
      shape = Type, 
      fill = "black"
    ), 
    size = 3
  ) + 
  theme_bw() +
  ggplot2::geom_smooth(
    method = "lm", 
    formula = my.formula, 
    col = "black"
  ) +
  ggpmisc::stat_poly_eq(
    formula = my.formula, col = "black",
    aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~")), 
    parse = TRUE
  ) + 
  xlab(label = "Log DNA concentration (ng)") + 
  ylab(label = "Cq-value") +
  theme(
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14),
    legend.position = "none"
  )

cq_plot

#ggsave(
#  filename = 'Supplemental_Figure_1a.pdf',
#  plot = cq_plot, 
#  device = 'pdf'
#)
```


### Supplemental Fig. 1b - Fragment Analyzer data

Here we plot an electropherogram generated using capillary gel electrophoresis of a
SiMSen-Seq library generated with the assay used in this study and positive
control DNA (PTC) as well as negative control (NTC), each in triplicate.

```{r, warning=FALSE}
# Import electropherogram data
electropherogram <- readr::read_csv(file = "data/tp1_electropherogram.csv")

# Calculate mean signal and reformat data for plotting
electropherogram <- electropherogram %>% dplyr::mutate(
    `MCF7 DNA` = rowMeans(select(electropherogram, starts_with("ptc")), na.rm = TRUE),
    NTC = rowMeans(select(electropherogram, starts_with("ntc")), na.rm = TRUE)
  ) %>% 
  dplyr::select(`Size (bp)`, `MCF7 DNA`, NTC) %>%
  tidyr::gather("Sample", "Fluorescence",-c(`Size (bp)`)) %>%
  dplyr::filter(
    `Size (bp)` > 0,
    `Size (bp)` < 200
  )

# Plot electropherogram
fa_plot <- ggplot(
  data = electropherogram,
  mapping = aes(
    x = `Size (bp)`,
    y = Fluorescence
    ),
  col = Sample
  ) + 
  geom_line(aes(col = Sample)) +
  theme_minimal() + 
  theme(
    axis.text = element_text(size = 12), 
    panel.border = element_blank(), 
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.line = element_line(colour = 'black'),
    axis.ticks.x = element_line(colour = 'black'),
    axis.ticks.y = element_line(colour = 'black'), legend.position = "bottom"
    ) 

fa_plot

#ggsave(
#  filename = 'Supplemental_Figure_1b.pdf',
#  plot = fa_plot, 
#  device = 'pdf'
#)

```


### Supplemental Fig. 1c

```{r, eval = FALSE}
# TODO this needs to be adapted to UMIErrorCorrect output
mean <- cons.table.filtered %>% 
  dplyr::summarise(
    mean_raw = mean(rawDepth),
    mean_cons = mean(consDepth)
    )

raw_reads_mean <- mean %>% select(mean_raw)
cons_reads_mean <- mean %>% select(mean_cons)

depths <- cons.table.filtered %>%
    tidyr::unite('Type', Manufacturer, InsertType, Purification, sep = ' ') %>%
    group_by(Type, Batch, Replicate) %>%
    mutate(
        relativeRaw = rawDepth/raw_reads_mean,
        relativeCons = consDepth/cons_reads_mean
    ) %>%
  dplyr::summarise(
    rawMean = mean(relativeRaw$mean_raw), 
    consMean = mean(relativeCons$mean_cons)
    ) %>% 
  ungroup() %>% group_by(Type, Batch) %>%
  dplyr::summarise(
    raw = mean(rawMean), 
    sd_raw = sd(rawMean),
    cons = mean(consMean),
    sd_cons = sd(consMean)
  )
  
# Re-level factors to move control DNA sample towards the end of the plot.
depths$Type <- forcats::fct_relevel(depths$Type, 'control Insert_1 MCF7', after = Inf)

# Plot Raw data (consensus 0)
raw_plot <- ggplot(depths, aes(x = Type, y = log2(raw), fill = Batch)) +
  geom_bar(
    stat = "identity",
    position = position_dodge(preserve = "single"), 
    col = "black", 
    width = 0.6
    ) +
  geom_errorbar(
    mapping = aes(ymin = log2(raw-sd_raw), ymax = log2(raw+sd_raw)), 
    position = position_dodge(
      width = 0.6, 
      preserve = "single"
    ), 
    width = 0.25
    ) +
  ylim(-3.5,2) + 
  ylab(label = "Relative raw depth (log2)") +
  scale_fill_manual(
    values = c("gray55", "#f6be4c", "#469ed4")
    ) + 
  theme_minimal() +
  theme(
    axis.text.x = element_text(
      hjust = 1,
      angle = 45,
      size = 9, 
      colour = "black"
    ),
    axis.text.y = element_text(
      size = 12, 
      colour = "black"
    ),
    panel.border = element_blank(), 
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.line = element_line(colour = 'black'),
    axis.title.x = element_blank(),
    legend.position = "bottom"
    ) +
  geom_hline(
    yintercept = 0,
    colour="black"
  ) 

# Plot Raw data (consensus 0 and consensus 10)
cons_plot <- ggplot(depths, aes(x = Type, y = log2(cons), fill = Batch)) +
  geom_bar(
    stat = "identity",
    position = position_dodge(preserve = "single"), 
    col = "black", 
    width = 0.6
    ) +
  geom_errorbar(
    mapping = aes(ymin = log2(cons-sd_cons), ymax = log2(cons+sd_cons)), 
    position = position_dodge(
      width = 0.6, 
      preserve = "single"
    ), 
    width = 0.25
    ) +
  ylim(-3.5,2) + 
  ylab(label = "Relative consensus 10 depth (log2)") +
  scale_fill_manual(
    values = c("gray55", "#f6be4c", "#469ed4")
    ) + 
  theme_minimal() +
  theme(
    axis.text.x = element_text(
      hjust = 1,
      angle = 45,
      size = 9, 
      colour = "black"
    ),
    axis.text.y = element_text(
      size = 12, 
      colour = "black"
    ),
    panel.border = element_blank(), 
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.line = element_line(colour = 'black'),
    axis.title.x = element_blank(),
    legend.position = "bottom"
    ) +
  geom_hline(
    yintercept = 0,
    colour="black"
  )

supp_figure_1 <- raw_plot + cons_plot
supp_figure_1

#ggsave(
#  filename = 'Supplemental_Figure_1.pdf',
#  plot = supp_figure_1, 
#  device = 'pdf', 
#  width = 12, 
#  height = 4
#)
```



## Supplemental Figure 2

**Error dependence on base position. (a)** Deletion (top) and substitution (bottom) 
errors are shown for batch 1 of Insert #1 type oligos. Each bar represents a position 
in the synthetic molecule (5' - 3'). Mean + SD is shown (n = 3 technical replicates). 
**(b)** Spearman correlations of consensus 10 errors (total error) with base position. 
All correlations were considered significant at p < 0.05.

```{r, echo=FALSE}
insertType = c('Insert_1', 'Insert_2')
batch = c('b1', 'b2', 'b3')

cons.variants <- cons.table.filtered %>%
    dplyr::mutate(
      subsitution_error = 100*(A+C+G+T)/Coverage,
      indel_error = 100*(D+I)/Coverage,
      total_error =  100*(A+C+G+T+D+I)/Coverage
    ) %>%
    dplyr::filter(
      `Consensus group size` == consensus_cutoff,
      InsertType %in% insertType,
      Batch %in% batch,
      Purification != "MCF7"
    ) %>%
    tidyr::unite('Type', Manufacturer, Purification,  sep = ' ') %>%
    dplyr::group_by(Type, Batch, InsertType) %>%
    dplyr::summarise(
      `Substitutions (%)` = mean(subsitution_error),
      subs_std = sd(subsitution_error),
      `Deletions (%)` = mean(indel_error),
      indels_std = sd(indel_error)
    )

df3 <- cons.variants %>% 
  dplyr::select(Type, Batch, InsertType, `Substitutions (%)`, `Deletions (%)`)
 
df3$Type <- forcats::fct_relevel(df3$Type, 'control MCF7', after = Inf)

fig <- df3 %>% tidyr::gather(Error,`Variant Allele Frequency (%)`,-c(Type,Batch,InsertType)) %>% 
  ggplot(aes(x = Type, y = `Variant Allele Frequency (%)`, fill = Error)) + 
  geom_bar(position = "dodge", stat = "identity") +
  geom_text(aes(label= round(`Variant Allele Frequency (%)`,2)), vjust=0) + 
  facet_grid(Batch ~ InsertType, scales = "free") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(
      hjust = 1,
      angle = 45,
      size = 9
    ),
    panel.border = element_blank(), 
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.line = element_line(colour = 'black'),
    legend.position = "bottom"
    ) +
  geom_hline(
    yintercept = 0,
    colour="gray55",
    linetype="dashed"
  ) 

fig

# Coefficients of variation
#cov_subs <- sd(df3$`Substitutions (%)`)/mean(df3$`Substitutions (%)`)
#cov_del <- sd(df3$`Deletions (%)`)/mean(df3$`Deletions (%)`)

#100-100*cov_subs/cov_del

#ggsave(filename = 'Figure_X_deletion_substitution_freq_batch_1.pdf', plot = fig, device = 'pdf')
#write.csv(x = df3, file = 'Table_X_deletion_substitution_freq_batch_1.csv')
```


### Supplemental Figure 3 - Error plot insert variant 1

Plot substitution and indel errors for remaining batches of insert variant 1.

```{r Figure2_supp, echo=FALSE, warning=FALSE}
fig <- perBaseErrorPlot(
  data = cons.table.filtered,
  insertType = c('Insert_1'),
  batch = c('b1', 'b2','b3'), 
  hide.legend = FALSE
)

fig

#ggsave(filename = 'Supplemental_Figure_2.pdf', plot = fig, device = 'pdf')
```

### Supplemental Figure 4 - Error plot insert variant 2

Plot substitution and indel errors for all batches of insert variant 2.

```{r Figure3_supp, echo=FALSE, warning=FALSE}
fig <- perBaseErrorPlot(
  data = cons.table.filtered,
  insertType = c('Insert_2'),
  batch = c('b1','b2','b3')
)

fig

#ggsave(filename = 'Supplemental_Figure_3.pdf', plot = fig, device = 'pdf')
```

## System information

This analysis was performed with the following system specifications:

```{r}
sessionInfo()
```


